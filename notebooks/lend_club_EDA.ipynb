{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff78dd50-7af7-4201-8452-e54fb133e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##**This notebook has two output csv files as of 6-02-25.  One for handling the imbalanced data with SMOTE and one without SMOTE**\n",
    "## Reason for this is because I want to test without smote and using class weights penalties with Keras so see if I get\n",
    "## better results.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "files = ['input.csv', 'output.csv', 'loan_data.csv']\n",
    "for fn in files:\n",
    "    path = f'../data/{fn}'     # note the ../ to go up one directory\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"{fn}: {df.shape}\")\n",
    "    display(df.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f649c743-fd56-47a3-bdcb-bccbcbe3dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Inspect structure & missing values\n",
    "df = pd.read_csv('../data/loan_data.csv')\n",
    "df.info()\n",
    "\n",
    "# 2. Quick summary stats\n",
    "df.describe()\n",
    "\n",
    "# 3. Examine the target balance\n",
    "print(df['not.fully.paid'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5f623-d482-4278-afd7-163a0259232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8754d71-b58b-4c51-a22c-6dd938f49b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['purpose'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f7f8d0-e600-4839-9b21-0779c24039a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these unique values for purpose all have a good amount in them so no need to group and I will just need to one-hot encode them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4ee36-1c47-40c6-b3ff-31bd361c6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Create engineered columns\n",
    "df['revol_bal_log']     = np.log1p(df['revol.bal'])\n",
    "df['cr_line_years']     = df['days.with.cr.line'] / 365\n",
    "df['cr_line_years_log'] = np.log1p(df['cr_line_years'])\n",
    "df['pub_rec_flag']      = (df['pub.rec'] > 0).astype(int)\n",
    "df['inquiry_rate']      = df['inq.last.6mths'] / df['cr_line_years']\n",
    "\n",
    "# 2. Plot distributions before/after and of new features\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 12))\n",
    "\n",
    "# Raw vs log revol.bal\n",
    "sns.histplot(df['revol.bal'], ax=axes[0,0], kde=True)\n",
    "axes[0,0].set_title('revol.bal (raw)')\n",
    "sns.histplot(df['revol_bal_log'], ax=axes[0,1], kde=True)\n",
    "axes[0,1].set_title('revol_bal_log (log1p)')\n",
    "\n",
    "# Raw vs log credit-line years\n",
    "sns.histplot(df['cr_line_years'], ax=axes[1,0], kde=True)\n",
    "axes[1,0].set_title('cr_line_years (raw)')\n",
    "sns.histplot(df['cr_line_years_log'], ax=axes[1,1], kde=True)\n",
    "axes[1,1].set_title('cr_line_years_log (log1p)')\n",
    "\n",
    "# New engineered features\n",
    "sns.histplot(df['pub_rec_flag'], ax=axes[2,0], kde=False)\n",
    "axes[2,0].set_title('pub_rec_flag')\n",
    "sns.histplot(df['inquiry_rate'], ax=axes[2,1], kde=True)\n",
    "axes[2,1].set_title('inquiry_rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c2b7f2-2be3-4164-8097-c81d5b4e6022",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Raw revol.bal skew:\", df['revol.bal'].skew())\n",
    "print(\"Log revol.bal skew:\", df['revol_bal_log'].skew())\n",
    "print(\"Raw cr_line_years skew:\", df['cr_line_years'].skew())\n",
    "print(\"Log cr_line_years skew:\", df['cr_line_years_log'].skew())\n",
    "# huge positive skews (>1) dropped to small negative values (near symmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e1ac56-5c57-41d3-9c2d-6ef5aa649d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# List the numeric columns to scale\n",
    "num_cols = [\n",
    "    'revol_bal_log',\n",
    "    'cr_line_years_log',\n",
    "    'int.rate',\n",
    "    'installment',\n",
    "    'log.annual.inc',\n",
    "    'dti',\n",
    "    'fico',\n",
    "    'revol.util',\n",
    "    'inq.last.6mths',\n",
    "    'delinq.2yrs',\n",
    "    'pub.rec',\n",
    "    'inquiry_rate'\n",
    "]\n",
    "\n",
    "# Initialize scaler and fit_transform on the entire df for now\n",
    "# (In practice, we'll do train/test split first—but for EDA/demo we can scale everything)\n",
    "scaler = StandardScaler()\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "# Quick sanity check: all means ~0, stds ~1\n",
    "df[num_cols].describe().loc[['mean','std']].T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e5cbe7-dd1d-4710-8f4c-2c58a15f7e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode purpose, drop the first category to avoid multicollinearity\n",
    "df = pd.get_dummies(df, columns=['purpose'], drop_first=True)\n",
    "\n",
    "# Check new columns\n",
    "[p for p in df.columns if p.startswith('purpose_')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d0bd1a-f6d8-45aa-a824-dea37b3188f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a csv of the transformed and clean data ready for the model\n",
    "df.to_csv('../data/loan_data_ready.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5760c54-070d-456d-bb7b-947c11cc8fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df_raw: ['credit.policy', 'int.rate', 'installment', 'log.annual.inc', 'dti', 'fico', 'revol.util', 'inq.last.6mths', 'delinq.2yrs', 'pub.rec', 'not.fully.paid', 'cr_line_years', 'pub_rec_flag', 'inquiry_rate', 'purpose_credit_card', 'purpose_debt_consolidation', 'purpose_educational', 'purpose_home_improvement', 'purpose_major_purchase', 'purpose_small_business']\n",
      "Sample scaled stats:\n",
      "                         mean       std\n",
      "int.rate       -4.747832e-17  1.000052\n",
      "installment    -4.896202e-17  1.000052\n",
      "log.annual.inc  1.348681e-15  1.000052\n",
      "dti            -7.121748e-17  1.000052\n",
      "fico            2.848699e-16  1.000052\n",
      "revol.util      4.154353e-17  1.000052\n",
      "inq.last.6mths  2.373916e-17  1.000052\n",
      "delinq.2yrs     1.186958e-17  1.000052\n",
      "pub.rec         5.638051e-17  1.000052\n",
      "cr_line_years  -6.528269e-17  1.000052\n",
      "inquiry_rate    0.000000e+00  1.000052\n"
     ]
    }
   ],
   "source": [
    "# **** Do-over this time transforming the csv differently to see if I get different results with same model *****\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Reload true raw data (unmodified loan_data.csv)\n",
    "df_raw = pd.read_csv('../data/loan_data.csv')\n",
    "\n",
    "# 2. Engineer features (except log‐transforms)\n",
    "df_raw['cr_line_years'] = df_raw['days.with.cr.line'] / 365\n",
    "df_raw['pub_rec_flag']  = (df_raw['pub.rec'] > 0).astype(int)\n",
    "df_raw['inquiry_rate']  = df_raw['inq.last.6mths'] / df_raw['cr_line_years']\n",
    "\n",
    "# 3. One‐hot encode 'purpose'\n",
    "df_raw = pd.get_dummies(df_raw, columns=['purpose'], drop_first=True)\n",
    "\n",
    "# 4. Drop the raw skewed columns (we want to keep them un‐logged but scaled)\n",
    "\n",
    "df_raw = df_raw.drop(columns=['revol.bal', 'days.with.cr.line'])\n",
    "\n",
    "# 5. Standard‐scale numeric features (no log columns here):\n",
    "num_cols_raw = [\n",
    "    'int.rate',\n",
    "    'installment',\n",
    "    'log.annual.inc',\n",
    "    'dti',\n",
    "    'fico',\n",
    "    'revol.util',\n",
    "    'inq.last.6mths',\n",
    "    'delinq.2yrs',\n",
    "    'pub.rec',\n",
    "    'cr_line_years',\n",
    "    'inquiry_rate'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler().fit(df_raw[num_cols_raw])\n",
    "df_raw[num_cols_raw] = scaler.transform(df_raw[num_cols_raw])\n",
    "\n",
    "# 6. Save as a new CSV variant\n",
    "df_raw.to_csv('../data/loan_data_ready_raw.csv', index=False)\n",
    "\n",
    "# 7. Quick verification\n",
    "print(\"Columns in df_raw:\", df_raw.columns.tolist())\n",
    "print(\"Sample scaled stats:\\n\", df_raw[num_cols_raw].describe().loc[['mean','std']].T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807ed66-04d6-4a10-a36f-8fddf74d1aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
